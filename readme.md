# Tritone Music - Unclaimed Works Analysis

This project is a submission for the **Tritone Analytics, Inc** Task. It analyzes an artist's catalog from Spotify and cross-references it with a provided dataset of unclaimed musical work shares to identify matching songs.

The script is optimized for performance, using efficient data structures, batched API requests, and a caching mechanism to avoid re-processing large files.

---

## Features

-   **Efficient Data Processing**: Reads a large TSV file and stores relevant data in a Set for instantaneous lookups.
-   **Smart Caching**: Processes the large dataset only once and creates a cache file for all subsequent runs, making execution after the first run incredibly fast.
-   **Optimized API Usage**: Fetches data from the Spotify API using batched requests to minimize network latency.
-   **Automated Report Generation**: Produces a multi-sheet Excel (`.xlsx`) file with the complete artist catalog, a list of matches, and detailed notes on the process.

---

## Setup and Installation

### 1. Prerequisites

-   Python 3.8+
-   A Spotify Developer account to get API credentials.

### 2. Clone the Repository

Clone the git repository.

```bash
git clone https://github.com/sasidharakurathi/tritone_task.git
cd tritone_task
```

### 3. Create a Virtual Environment

It's highly recommended to use a virtual environment to manage dependencies.

```bash
# Create the virtual environment
python -m venv .venv

# Activate it
# On macOS/Linux:
source venv/bin/activate
# On Windows:
.\.venv\Scripts\activate
```

### 4. Install Dependencies

Install the required Python libraries using the `requirements.txt` file.

```bash
pip install -r requirements.txt
```

### 5. Set Up Environment Variables

The script requires your Spotify API credentials. Create a file named `.env` in the root of the project folder and add your keys to it.

```
# .env file

SPOTIPY_CLIENT_ID="Your_Spotify_Client_Id_Goes_Here"
SPOTIPY_CLIENT_SECRET="Your_Spotify_Client_Secret_Goes_Here"
```

### 6. Place the Dataset

Download the `unclaimedmusicalworkrightshares.tsv` file and place it in the same directory as the script.

The dataset used for this project (`unclaimedmusicalworkrightshares.tsv`) is too large to be stored in this repository.

**You can download it from the following link:**

[**Download the Dataset from OneDrive**](https://ashthegamer-my.sharepoint.com/:u:/g/personal/ash_ashthegamer_onmicrosoft_com/ESIZxJ92wDtGpINoKphx8PsB65L1WX5Im5wILJRXR3hbWg?e=JdHei1)

Please download the file and place it in the root directory of this project before running the script.

---

## How to Run the Script

Once the setup is complete, you can run the main script from your terminal:

```bash
python main.py
```

### First Run

The first time you run the script, it will take a few minutes to process the large `unclaimedmusicalworkrightshares.tsv` file and create a cache file (`isrc_cache.pkl`).

### Subsequent Runs

Every run after the first will be significantly faster, as the script will load the processed data directly from the cache file.

---

## Project Structure

```
tritone-task/
├── .env                  # Stores your secret API keys
├── main.py               # The main Python script
├── README.md             # This file
├── requirements.txt      # List of Python dependencies
├── unclaimedmusicalworkrightshares.tsv # The large dataset file (not included in repo)
├── isrc_cache.pkl        # The cache file, generated after the first run
└── tritone_task_output.xlsx  # The final Excel report, generated by the script
```

---

## Technical Approach

-   **Caching**: To avoid re-processing the 2GB+ TSV file on every run, the script serializes the resulting Set of ISRCs into a `.pkl` file using the `pickle` module. This cache is automatically loaded if it exists.
-   **Efficient Lookups**: A Python `Set` is used to store the ISRC codes, providing O(1) average time complexity for the cross-referencing logic.
-   **Batch API Requests**: The `spotipy` library is used to fetch track details from Spotify in batches of 50, which is the most efficient way to reduce the number of network requests and minimize API rate-limiting issues.